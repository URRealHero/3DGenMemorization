name: "CustomDiT_Text_Top16_NoAug_FlowMatching_1024x32"

sampling_common: &sampling_common
  bounds: [-1.01, -1.01, -1.01, 1.01, 1.01, 1.01]
  octree_depth: 8        # callback will convert to octree_resolution=256
  num_chunks: 50000
  mc_level: 0.0

training:
  steps: 500000 # 600000
  use_amp: true
  amp_type: "bf16"
  base_lr: 1e-4 # 1e-4
  gradient_clip_val: 1.0
  gradient_clip_algorithm: "norm"
  every_n_train_steps: 5000
  val_check_interval: 4999   # How often to run validation
  limit_val_batches: 4
  update_every: 2

dataset:
  target: hy3dshape.data.datamodule.UnifiedDataModule
  params:
    mode: "latent" # "raw"
    batch_size: 64 # global batch size 512 (for baseline, we train under 4 H100, with gradient accumulation 2)
    num_workers: 8
    max_dataset_size: 51200
    dataset_type: "objaverse_lvis"
    
    # --- Fill these in with your actual paths ---
    # dataset_folder:
    # val_dataset_folder:
    train_latent_folder: PATH/TO/TRAIN_LATENTS
    val_latent_folder:  PATH/TO/VAL_LATENTS


    train_csv_path: utils/lvis/top16/train.csv
    val_csv_path: utils/lvis/top16/val.csv

    conditioning_type: "text" # Can be "class" or "uncond"
    snapshot_dir: ./splits/16-class # used for caching the training data.

model:
  target: hy3dshape.models.diffusion.flow_matching_sit.Diffuser
  params:
    scale_by_std: true
    z_scale_factor: 1.0 # will be re-set by first batch statistics
    torch_compile: true
    
    first_stage_config:
      target: hy3dshape.models.autoencoders.autoencoder.CustomShapeVAE
      params:
        # --- Parameters for your VAE ---
        model_depth: 24
        model_dim: 1024
        surface_size: 8192
        num_latents: 1024
        latent_dim: 32
        query_type: 'point'
        bottleneck: 'normalized'

        pretrained:
          ckpt_path: '/PATH/TO/point_vec1024x32_dim1024_depth24_sdf_nb/checkpoint-110.pth' # https://github.com/1zb/VecSetX
          ckpt_key: "model"
          strip_prefixes: ["module."]
          target_submodule: "autoencoder"

    cond_stage_config:
      target: hy3dshape.models.conditioner.MultiConditioner
      params:
        type: "text" # Must match dataset.conditioning_type
        # If type: "class", you would need to add:
        # num_classes: 16 
        text_embed_dim: 512
        p_uncond: 0.1

    denoiser_cfg:
      target: hy3dshape.models.denoisers.dit.DiTPlain
      params:
        # ✅ VAE latent shape
        in_channels: 32   # Your latent_dim
        max_seq_len: 1024 # Your num_latents
        
        # ✅ Text embedding shape (CLIP ViT-B/16)
        context_dim: 512
        text_len: 77
        
        # --- Model architecture (can be tuned) ---
        hidden_size: 1024
        depth: 12
        num_heads: 16
        qk_norm: true
        use_attention_pooling: false
        use_pos_emb: false
        qk_norm_type: 'rms'
        num_moe_layers: 3
        num_experts: 4
        moe_top_k: 2 

    scheduler_cfg:
      transport:
        target: hy3dshape.models.diffusion.transport.create_transport
        params:
          path_type: Linear
          prediction: velocity
      sampler:
        target: hy3dshape.models.diffusion.transport.Sampler
        params: {}
        ode_params:
          sampling_method: euler
          num_steps: &num_steps 50


    optimizer_cfg:
      # NOT TRAINABLE CONDITIONING ENCODER
      # train_image_encoder: true
      # image_encoder_lr_multiply: 0.1 
      optimizer:
        target: torch.optim.AdamW
        params:
          fused: true
          betas: [0.9, 0.99]
          eps: 1.e-6
          weight_decay: 1.e-2
      scheduler:
        target: hy3dshape.utils.trainings.lr_scheduler.LambdaWarmUpCosineFactorScheduler
        params:
          warm_up_steps: 500
          f_start: 1.e-6
          f_min: 1.e-3
          f_max: 1.0
          
    pipeline_cfg:
      target: hy3dshape.pipelines.CustomFlowMatchingPipeline

callbacks:
  mesh_logger:
    target: hy3dshape.utils.trainings.mesh_log_callback.ConditionalMeshLogger
    params:
      step_frequency: 4999
      num_samples: 4
      <<: *sampling_common


  # fpd via training
  # fpd_eval:
  #   target: hy3dshape.utils.trainings.fpd_eval_callback.ModelEvalFPDCallback
  #   params:
  #     distributed: true
  #     test_csv: utils/lvis/top16/objaverse_ext_val_top100_wo_buildings_top16.csv
  #     cache_dir: /scratch/gpfs/sp2526/fpd_cache_top16
  #     ref_npz_root: /scratch/gpfs/ZHUANGL/sp2526/Objaverse/lvis/Objaverse_ext_sampling
  #     ref_npz_key: surface_points
  #     reference_sample_points: 4096

  #     # encoder
  #     pointnet_ckpt: /scratch/gpfs/sp2526/AE_ckpt/pointnet/pointnet.pt
  #     width_mult: 2
  #     device_batch_size: 64
  #     normal_channel: false

  #     # eval sizing
  #     n_eval: 500
  #     n_points: 4096

  #     # conditioning (always LVIS)
  #     condition_type: text
  #     lvis_map_json: null

  #     # when to run
  #     heavy_every_n_steps: 20000
  #     also_run_on_validation: false

  #     # sampling → meshing kwargs
  #     sampler: *sampling_common   